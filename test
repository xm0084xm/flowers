import pandas as pd
import numpy as np
import mglearn
import matplotlib.pyplot as plt
import pymysql
import os
9
from mpl_toolkits.mplot3d import Axes3D,axes3d
from IPython.display import display
from sklearn.metrics import mean_squared_error
#网格管道
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
#预处理:转换缩放排列分割填空
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
#编码
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
#分类
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
#回归
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
#交叉验证
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import LeaveOneOut  
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GroupKFold
#原数据:定义标签y,全集，关键集，带标签关键集
waste = pd.read_csv(r"data\waste.csv")
waste_labels=waste.Type.copy()
waste_num=waste.drop("Type",axis=1)
waste_numcut=waste.drop(["Type","long","width","People","Area","Rooms","daytotal","daymean"],axis=1)
waste_numCUT=waste.drop(["long","width","People","Area","Rooms","daytotal","daymean"],axis=1)
#新数据:定义标签y,全集，关键集
waste_new = pd.read_csv(r"data\waste_new.csv")
waste_new_labels=waste_new.Type.copy()
waste_new_num=waste_new.drop("Type",axis=1)
waste_new_numcut=waste_new.drop(["Type","long","width","People","Area","Rooms","daytotal","daymean"],axis=1)
print("垃圾分类：0有害，1厨余，2回收，3其他")

#近邻分类
X_train, X_test, y_train, y_test = train_test_split (waste_numcut, waste_labels, test_size=0.25, random_state=3)
knn1=KNeighborsClassifier(n_neighbors=1).fit(X_train,y_train)
print("关键集:近邻预测4个新样本分类: {}".format(knn1.predict(waste_new_numcut)))
print("关键集:近邻分类训练分数: {}".format(knn1.score(X_train,y_train)))
print("关键集:近邻分类测试分数: {}".format(knn1.score(X_test,y_test)))

X_train,X_test,y_train,y_test=train_test_split(waste_num,waste_labels,test_size=0.25,random_state=3)
knn2=KNeighborsClassifier(n_neighbors=1).fit(X_train,y_train)
print("全集:近邻预测4个新样本分类: {}".format(knn2.predict(waste_new_num)))
print("全集:近邻分类训练分数: {}".format(knn2.score(X_train,y_train)))
print("全集:近邻分类测试分数: {}".format(knn2.score(X_test,y_test)))

#线性回归
X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
linr1=LinearRegression().fit(X_train,y_train)
print("关键集:线性回归训练分: {}".format(linr1.score(X_train,y_train)))
print("关键集:线性回归测试分: {}".format(linr1.score(X_test,y_test)))

X_train,X_test,y_train,y_test=train_test_split(waste_num,waste_labels,test_size=0.25,random_state=3)
linr2=LinearRegression().fit(X_train,y_train)
print("全集:线性回归训练分: {}".format(linr2.score(X_train,y_train)))
print("全集:线性回归测试分: {}".format(linr2.score(X_test,y_test)))

#岭回归
def ridgescore_waste_numcut():
    X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
    D=[0.0001,0.001,0.01,0.1,1,10,100]
    for alpha in D:
        ridge1=Ridge(alpha=alpha).fit(X_train,y_train)
        print("关键集:岭回归训练分:a={:<12f}   {} ".format(alpha,ridge1.score(X_train,y_train)))
        print("关键集:岭回归测试分:a={:<12f}   {} ".format(alpha,ridge1.score(X_test,y_test)))
sa=ridgescore_waste_numcut()
print(sa)

def ridgescore_waste_num():
    X_train,X_test,y_train,y_test=train_test_split(waste_num,waste_labels,test_size=0.25,random_state=3)
    D=[0.0001,0.001,0.01,0.1,1,10,100]
    for alpha in D:
        ridge2=Ridge(alpha=alpha).fit(X_train,y_train)
        print("全集:岭回归训练分:a={:<12f}   {} ".format(alpha,ridge2.score(X_train,y_train)))
        print("全集:岭回归测试分:a={:<12f}   {} ".format(alpha,ridge2.score(X_test,y_test)))
sb=ridgescore_waste_num()
print(sb)

#lasso回归
def lasso_waste_numcut():
    X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
    D=[0.0001,0.001,0.01,0.1,1,10,100]
    for alpha in D:
        lasso1=Lasso(max_iter=100000,alpha=alpha).fit(X_train,y_train)
        print("关键集:lasso回归训练分:a={:<12f},   {} ".format(alpha,lasso1.score(X_train,y_train)))
        print("关键集:lasso回归测试分:a={:<12f},   {} ".format(alpha,lasso1.score(X_test,y_test)))
sc=lasso_waste_numcut()
print(sc)

def lasso_waste_num():
    X_train,X_test,y_train,y_test=train_test_split(waste_num,waste_labels,test_size=0.25,random_state=3)
    D=[0.0001,0.001,0.01,0.1,1,10,100]
    for alpha in D:
        lasso2=Lasso(max_iter=100000,alpha=alpha).fit(X_train,y_train)
        print("全集:lasso回归训练分:a={:<12f},   {} ".format(alpha,lasso2.score(X_train,y_train)))
        print("全集:lasso回归测试分:a={:<12f},   {} ".format(alpha,lasso2.score(X_test,y_test)))
sd=lasso_waste_num()
print(sd)


#搜索逻辑回归最佳参数

X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
log=LogisticRegression().fit(X_train,y_train)
pipe1 = Pipeline([('preprocessing', StandardScaler()), ('classifier',LogisticRegression())])
param_grid = [{'classifier': [LogisticRegression()], 'preprocessing': [StandardScaler(), None],'classifier__C': [0.01, 0.1, 1, 10]}]
grid = GridSearchCV(pipe1, param_grid, cv=4)
grid.fit(X_train, y_train)
print("关键集:逻辑回归最好参数为: {}".format(grid.best_params_))
print("关键集:逻辑回归最好分数为: {}".format(grid.best_score_))
print("关键集:逻辑回归最好训练分为: {}".format(grid.score(X_train, y_train)))
print("关键集:逻辑回归最好测试分为: {}".format(grid.score(X_test, y_test)))

#搜索支持向量机最佳参数
X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
svc=SVC().fit(X_train,y_train)
pipe2 = Pipeline([('preprocessing', StandardScaler()), ('classifier',SVC())])
param_grid = [{'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],'classifier__C': [0.01, 0.1, 1, 10]}]
grid = GridSearchCV(pipe2, param_grid, cv=4)
grid.fit(X_train, y_train)
print("关键集:支持向量机最好参数为: {}".format(grid.best_params_))
print("关键集:支持向量机最好分数为: {}".format(grid.best_score_))
print("关键集:支持向量机最好训练分为: {}".format(grid.score(X_train, y_train)))
print("关键集:支持向量机最好测试分为: {}".format(grid.score(X_test, y_test)))

#搜索随机森林最佳参数
X_train,X_test,y_train,y_test=train_test_split(waste_numcut,waste_labels,test_size=0.25,random_state=3)
rfc=RandomForestClassifier().fit(X_train,y_train)
pipe3 = Pipeline([('preprocessing', StandardScaler()), ('classifier',RandomForestClassifier())])
param_grid = [{'classifier': [RandomForestClassifier(n_estimators=100)],'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]
grid = GridSearchCV(pipe3, param_grid, cv=4)
grid.fit(X_train, y_train)
print("关键集:随机森林最好参数为: {}".format(grid.best_params_))
print("关键集:随机森林最好分数为: {}".format(grid.best_score_))
print("关键集:随机森林最好训练分为: {}".format(grid.score(X_train, y_train)))
print("关键集:随机森林最好测试分为: {}".format(grid.score(X_test, y_test)))

#2D绘图
for s,label in zip(["People","Area","Rooms","daytotal"],["People","Area","Rooms","daytotal"]):
    waste.plot(xticks=[20,40,60,70],yticks=[-20,0,20,40,60,80,100],kind="scatter",x="width",y="long",alpha=0.9, s=s ,label=label,figsize=(10,8),c="SU", cmap=plt.get_cmap("jet"), colorbar=True,)
    plt.xlabel=("width")
    plt.ylabel=("long")  
plt.legend()
plt.show()
#下图中可以发现一些桶数高低规律：红色一般是厨余，黄色一般是其他，黄绿色一般是回收，蓝色一般是有害。

#图3表示小区的楼房房间数都很均匀，西边用户群喜欢丢弃大量厨余和其他垃圾，桶数较高，东边的用户群丢弃的较少。

#图1表示在小区均匀分布房间的楼房中，西边和南边用户群住户人数较多，垃圾的丢弃量也相对较多。

#图2表示垃圾桶辐射区域很均匀，但桶数差异较大，垃圾分布不均匀，需要因地制宜往西边用户增加垃圾桶数量。

#图4的环形表示多类垃圾相加，可以看出西边用户喜欢丢弃较多厨余和有害，东边用户喜欢丢弃其他和有害，中间和南边的用户喜欢丢弃回收和厨余。

#3D绘图
x_new = np.hstack([waste_numCUT.values, waste_numCUT.values[:, 1:] ** 2])
figure=plt.figure()
ax=Axes3D(figure,elev=-150,azim=-30)
y=waste_labels.values
mask = y == 0
ax.scatter(x_new[mask, 0], x_new[mask, 1], x_new[mask, 2], c='b',cmap=mglearn.cm2, s=60)
ax.scatter(x_new[~mask, 0], x_new[~mask, 1], x_new[~mask, 2], c='g', marker='o',cmap=mglearn.cm2, s=60)
ax.set_xlabel("feature0")
ax.set_ylabel("feature1")
ax.set_zlabel("feature1 ** 2")


